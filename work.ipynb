{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym environment and Implementing Reinforcement Learning Agents \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Introduction\n",
    "\n",
    "The **Bipedal Walker** environment is part of the Gymnasium library and is a fascinating and challenging problem in the field of reinforcement learning. \n",
    "\n",
    "It requires the control of a simulated robot with **four motorized joints**, guiding it to walk efficiently across uneven terrains. This project focuses on leveraging reinforcement learning techniques to train an agent capable of mastering this environment.  \n",
    "\n",
    "\n",
    "### Why This Project?  \n",
    "The decision to work on the Bipedal Walker environment was driven by several key motivations:  \n",
    "1. **Real-World Relevance**:  \n",
    "   The principles behind this task are directly applicable to real-world robotics, especially in the domains of bipedal locomotion and autonomous navigation over rough terrains. \n",
    "\n",
    "2. **Challenge of Continuous Control**:  \n",
    "   Unlike simple discrete-action environments, the Bipedal Walker requires handling a **continuous action space**, where precise motor control is essential for success. This adds complexity and provides an excellent opportunity to explore advanced reinforcement learning techniques.  \n",
    "\n",
    "3. **Rich Dynamics**:  \n",
    "   The environment simulates realistic physical dynamics, including gravity, friction, and inertia. \n",
    "\n",
    "\n",
    "## escolher dentro destes tópicos o melhor\n",
    "\n",
    "We believe the challenges in this project will revolve around the inherent complexity of the Bipedal Walker environment and the demands of training an effective reinforcement learning agent.\n",
    "One major challenge will be efficient exploration. The environment requires precise control of the robot's motors to balance, walk, and navigate uneven terrains. Early episodes are likely to result in the robot falling frequently, making it difficult for the agent to explore and learn effectively.\n",
    "\n",
    "Another significant difficulty is the presence of sparse rewards. The agent is mainly rewarded for moving forward, which means the reward signals are infrequent. This can hinder the learning process, particularly in the early stages. Designing an effective reward strategy without introducing bias will be crucial.\n",
    "\n",
    "The continuous action space adds another layer of complexity. Unlike discrete-action environments, where the agent selects actions from a finite set, the Bipedal Walker requires precise motor control through continuous values. Solving this problem will require advanced algorithms like Proximal Policy Optimization (PPO) or Deep Deterministic Policy Gradient (DDPG).\n",
    "Additionally, the physics-based dynamics of the environment introduce realistic but challenging factors, such as gravity, friction, and inertia. Small control errors can lead to cascading failures, such as the robot falling or losing momentum. This makes the task more sensitive to precise tuning and optimization.\n",
    "\n",
    "The computational demands of the project should not be underestimated. Simulating the environment and training a reinforcement learning agent often requires significant computational power. Achieving consistent results might involve running multiple experiments to tune hyperparameters, further increasing the computational cost.\n",
    "\n",
    "\n",
    "\n",
    "### reward \n",
    "\n",
    "Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points, more optimal agent will get better score. State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There's no coordinates in the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descrição da imagem](./images/bipedal_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Virtual Display \n",
    "\n",
    "Please pay attention to the operative system you are using when running this code -- mudar isto? nem sei se está a rodar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start virtual display - WINDOWS\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start virtual display - MAC\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "env.render()  # Aqui, o ambiente será renderizado diretamente no macOS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space\n",
    "\n",
    "Our agent should fit to whatever it is, but it's better to know the inputs if our agent is not learning. \n",
    "\n",
    "Here is the observation table from [this link](https://github.com/openai/gym/wiki/BipedalWalker-v2), with 24 different parameters in one state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Shape (24,)\n",
      "Sample observation [ 2.7958372   4.2719393  -3.098417    3.5200021  -1.5043272   0.17990619\n",
      " -1.7329019  -2.693569    0.8306578   2.8606198   3.5075529  -1.6949596\n",
      "  4.8863325   3.932315   -0.344987    0.8760876   0.79466033 -0.07690509\n",
      " -0.40360272 -0.35994315  0.80476236 -0.15696786  0.04681255  0.05192911]\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descrição da imagem](./images/observation_space.png)\n",
    "\n",
    "\n",
    "\n",
    "According to [this link](https://colab.research.google.com/github/BrutFab/ppo_BipedalWalker_v3/blob/main/ppo_BipedalWalker_v3.ipynb#scrollTo=xr8LRvHOLxIx),Observation Space is a vector of size 24 and each value contains different information about the walker: \n",
    "\n",
    "\n",
    "- **Hull Angle Speed**: The speed at which the main body of the walker is rotating.\n",
    "- **Angular Velocity**: The rate of change of the angular position of the walker.\n",
    "- **Horizontal Speed**: The speed at which the walker is moving horizontally.\n",
    "- **Vertical Speed**: The speed at which the walker is moving vertically.\n",
    "- **Position of Joints**: The positions (angles) of the walker's joints. Given that the walker has 4 joints, this take up 4 values.\n",
    "- **Joints Angular Speed**: The rate of change of the angular position for each joint. Again, this would be 4 values for the 4 joints.\n",
    "- **Legs Contact with Ground**: Indicating whether each leg is in contact with the ground. Given two legs, this contains 2 values.\n",
    "- **10 Lidar Rangefinder Measurements**: These are distance measurements to detect obstacles or terrain features around the walker. There are 10 of these values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space \n",
    "\n",
    "BipedalWalker has two legs. Each leg has two joints. You have to teach the Bipedal-walker to walk by applying the torque on these joints.\n",
    "\n",
    " Therefore the size of our action space is four which is the torque applied on four joints. \n",
    "\n",
    " Actions are motor speed values in the [-1, 1] range for each of the 4 joints at both hips and knees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Shape (4,)\n",
      "Action Space Sample [-0.43058464  0.9464465  -0.2230738  -0.30005386]\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space Shape\", env.action_space.shape)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descrição da imagem](./images/action_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_id, hardcore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Environment\n",
    "\n",
    "> **Vectorized Environments** are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Because of this, actions passed to the environment are now a vector (of dimension n). It is the same for observations, rewards and end of episode signals (dones). In the case of non-array observation spaces such as Dict or Tuple, where different sub-spaces may have different shapes, the sub-observations are vectors (of dimension n).\n",
    "\n",
    "*This definition was taken from the official Stable Baselines 3 documentation. For more details, visit the [official guide on vectorized environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "env = make_vec_env('BipedalWalker-v3', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use 16 environments in parallel (n_envs=16) to speed up the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 2048,\n",
    "    batch_size = 128,\n",
    "    n_epochs = 6,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GymEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
